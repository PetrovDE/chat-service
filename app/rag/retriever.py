# app/rag/retriever.py

import logging
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document
from app.rag.vector_store import VectorStoreManager
from app.rag.document_loader import DocumentLoader
from app.rag.text_splitter import SmartTextSplitter
from app.rag.embeddings import embeddings_manager

logger = logging.getLogger(__name__)


class RAGRetriever:
    def __init__(self,
                 vectorstore: Optional[VectorStoreManager] = None,
                 documentloader: Optional[DocumentLoader] = None,
                 textsplitter: Optional[SmartTextSplitter] = None):
        self.vectorstore = vectorstore or VectorStoreManager()
        self.documentloader = documentloader or DocumentLoader()
        self.textsplitter = textsplitter or SmartTextSplitter()
        logger.info("RAGRetriever initialized")

    async def process_and_store_file(self, filepath: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        docs = await self.documentloader.load_file(filepath, metadata)
        chunk_docs = self.textsplitter.split_documents(docs)
        embeddings = embeddings_manager.embedd_documents([doc.page_content for doc in chunk_docs])
        for doc, emb in zip(chunk_docs, embeddings):
            chunk_id = doc.metadata.get("id", None) or doc.page_content[:40]
            self.vectorstore.add_document(doc_id=chunk_id, embedding=emb, metadata=doc.metadata)
        return {"count_stored_chunks": len(chunk_docs)}

    def query_rag(self, query_content: str, top_k: int = 5, user_id: Optional[str] = None) -> List[Document]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π content
        """
        embedding_query = embeddings_manager.embedd_documents([query_content])[0]
        results = self.vectorstore.query(embedding_query, top_k=top_k)
        logger.info(f"üîç Vector store returned {len(results)} raw results")

        if user_id:
            filtered_results = [r for r in results if r.get('metadata', {}).get('user_id') == user_id]
            logger.info(f"üîç After user_id filter: {len(filtered_results)} results")
            results = filtered_results

        documents = []
        for idx, result in enumerate(results):
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 6: –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è content
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content = None

            # –ò—Å—Ç–æ—á–Ω–∏–∫ 1: –ø—Ä—è–º–æ–µ –ø–æ–ª–µ content
            if 'content' in result and result['content']:
                content = result['content']

            # –ò—Å—Ç–æ—á–Ω–∏–∫ 2: content –≤ metadata
            elif 'metadata' in result and 'content' in result['metadata']:
                content = result['metadata']['content']

            # –ò—Å—Ç–æ—á–Ω–∏–∫ 3: page_content –≤ metadata
            elif 'metadata' in result and 'page_content' in result['metadata']:
                content = result['metadata']['page_content']

            # –ò—Å—Ç–æ—á–Ω–∏–∫ 4: text –≤ metadata
            elif 'metadata' in result and 'text' in result['metadata']:
                content = result['metadata']['text']

            if not content or not str(content).strip():
                logger.warning(
                    f"‚ö†Ô∏è Empty content for result {idx}, id={result.get('id')}, "
                    f"metadata keys: {list(result.get('metadata', {}).keys())}"
                )
                continue

            # –°–æ–∑–¥–∞–µ–º metadata –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è content
            metadata = result.get('metadata', {}).copy()

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            metadata['result_index'] = idx
            metadata['similarity_score'] = result.get('distance', 0)

            doc = Document(
                page_content=str(content),
                metadata=metadata
            )
            documents.append(doc)

            logger.debug(
                f"‚úÖ Document {idx}: {len(content)} chars, "
                f"file={metadata.get('filename', 'unknown')}"
            )

        logger.info(f"‚úÖ Returning {len(documents)} valid documents for RAG")

        if not documents:
            logger.warning("‚ö†Ô∏è No valid documents found - RAG context will be empty!")

        return documents

    def build_context_prompt(self, query: str, context_documents: List[Document]) -> str:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
        """
        if not context_documents:
            logger.warning("‚ö†Ô∏è No context documents provided - using query only")
            return query

        context_chunks = []
        for i, doc in enumerate(context_documents, 1):
            content = doc.page_content

            if not content or not content.strip():
                logger.warning(f"‚ö†Ô∏è Skipping document {i} - empty content")
                continue

            filename = doc.metadata.get('filename', 'Unknown')
            file_type = doc.metadata.get('file_type', '')
            chunk_index = doc.metadata.get('chunk_index', '')

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_header = f"[–î–æ–∫—É–º–µ–Ω—Ç {i} - {filename}"
            if file_type:
                doc_header += f" ({file_type})"
            if chunk_index is not None and chunk_index != '':
                doc_header += f" - —á–∞—Å—Ç—å {chunk_index + 1}"
            doc_header += "]"

            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 7: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            # —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏
            max_content_length = 2000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "\n[... —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ ...]"

            context_chunks.append(f"{doc_header}\n{content}")

        if not context_chunks:
            logger.warning("‚ö†Ô∏è All documents were empty - using query only")
            return query

        context_text = "\n\n---\n\n".join(context_chunks)

        prompt = f"""–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ.

–ö–û–ù–¢–ï–ö–°–¢ ({len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):
{context_text}

–í–û–ü–†–û–°:
{query}

–û–¢–í–ï–¢:"""

        logger.info(
            f"üìù Built prompt: {len(context_documents)} docs, "
            f"{len(context_text)} context chars, "
            f"{len(prompt)} total chars"
        )
        return prompt


rag_retriever = RAGRetriever()
