# app/rag/retriever.py
import logging
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document

from app.rag.vector_store import VectorStoreManager
from app.rag.document_loader import DocumentLoader
from app.rag.text_splitter import SmartTextSplitter
from app.rag.embeddings import embeddings_manager

logger = logging.getLogger(__name__)


class RAGRetriever:
    def __init__(
            self,
            vectorstore: Optional[VectorStoreManager] = None,
            documentloader: Optional[DocumentLoader] = None,
            textsplitter: Optional[SmartTextSplitter] = None
    ):
        self.vectorstore = vectorstore or VectorStoreManager()
        self.documentloader = documentloader or DocumentLoader()
        self.textsplitter = textsplitter or SmartTextSplitter()
        logger.info("RAGRetriever initialized")

    async def process_and_store_file(
            self,
            filepath: str,
            metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        docs = await self.documentloader.load_file(filepath, metadata)
        chunk_docs = self.textsplitter.split_documents(docs)
        embeddings = embeddings_manager.embedd_documents([doc.page_content for doc in chunk_docs])

        for doc, emb in zip(chunk_docs, embeddings):
            chunk_id = doc.metadata.get("id", None) or doc.page_content[:40]
            self.vectorstore.add_document(doc_id=chunk_id, embedding=emb, metadata=doc.metadata)

        return {"count_stored_chunks": len(chunk_docs)}

    def query_rag(
            self,
            query_content: str,
            top_k: int = 5,
            user_id: Optional[str] = None,
            conversation_id: Optional[str] = None  # ‚úÖ –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
    ) -> List[Document]:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ conversation_id
        """
        embedding_query = embeddings_manager.embedd_documents([query_content])[0]
        results = self.vectorstore.query(embedding_query, top_k=top_k)

        logger.info(f"üîç Vector store returned {len(results)} raw results")

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ conversation_id (–ì–õ–ê–í–ù–û–ï!)
        if conversation_id:
            filtered_results = [
                r for r in results
                if r.get('metadata', {}).get('conversation_id') == conversation_id
            ]
            logger.info(f"üîç After conversation_id filter: {len(filtered_results)} results")
            results = filtered_results

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ user_id (–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
        if user_id:
            filtered_results = [
                r for r in results
                if r.get('metadata', {}).get('user_id') == user_id
            ]
            logger.info(f"üîç After user_id filter: {len(filtered_results)} results")
            results = filtered_results

        documents = []
        for idx, result in enumerate(results):
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ content
            content = None

            if 'content' in result and result['content']:
                content = result['content']
            elif 'metadata' in result and 'content' in result['metadata']:
                content = result['metadata']['content']
            elif 'metadata' in result and 'page_content' in result['metadata']:
                content = result['metadata']['page_content']
            elif 'metadata' in result and 'text' in result['metadata']:
                content = result['metadata']['text']

            if not content or not str(content).strip():
                logger.warning(
                    f"‚ö†Ô∏è Empty content for result {idx}, id={result.get('id')}, "
                    f"metadata keys: {list(result.get('metadata', {}).keys())}"
                )
                continue

            metadata = result.get('metadata', {}).copy()
            metadata['result_index'] = idx
            metadata['similarity_score'] = result.get('distance', 0)

            doc = Document(
                page_content=str(content),
                metadata=metadata
            )
            documents.append(doc)

            logger.debug(
                f"‚úÖ Document {idx}: {len(content)} chars, "
                f"file={metadata.get('filename', 'unknown')}, "
                f"conv_id={metadata.get('conversation_id', 'none')}"
            )

        logger.info(f"‚úÖ Returning {len(documents)} valid documents for RAG")
        if not documents:
            logger.warning("‚ö†Ô∏è No valid documents found - RAG context will be empty!")

        return documents

    def build_context_prompt(self, query: str, context_documents: List[Document]) -> str:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
        """
        if not context_documents:
            logger.warning("‚ö†Ô∏è No context documents provided - using query only")
            return query

        context_chunks = []
        for i, doc in enumerate(context_documents, 1):
            content = doc.page_content
            if not content or not content.strip():
                logger.warning(f"‚ö†Ô∏è Skipping document {i} - empty content")
                continue

            filename = doc.metadata.get('filename', 'Unknown')
            file_type = doc.metadata.get('file_type', '')
            chunk_index = doc.metadata.get('chunk_index', '')

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_header = f"[–î–æ–∫—É–º–µ–Ω—Ç {i} - {filename}"
            if file_type:
                doc_header += f" ({file_type})"
            if chunk_index is not None and chunk_index != '':
                doc_header += f" - —á–∞—Å—Ç—å {chunk_index + 1}"
            doc_header += "]"

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            max_content_length = 2000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "\n[... —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ ...]"

            context_chunks.append(f"{doc_header}\n{content}")

        if not context_chunks:
            logger.warning("‚ö†Ô∏è All documents were empty - using query only")
            return query

        context_text = "\n\n---\n\n".join(context_chunks)

        prompt = f"""–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ.

–ö–û–ù–¢–ï–ö–°–¢ ({len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):
{context_text}

–í–û–ü–†–û–°:
{query}

–û–¢–í–ï–¢:"""

        logger.info(
            f"üìù Built prompt: {len(context_documents)} docs, "
            f"{len(context_text)} context chars, "
            f"{len(prompt)} total chars"
        )

        return prompt


rag_retriever = RAGRetriever()
