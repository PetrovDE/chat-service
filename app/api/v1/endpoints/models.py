# app/api/v1/endpoints/models.py
from fastapi import APIRouter, HTTPException
from typing import Dict, Any, List
import httpx
import logging
from app.core.config import settings
from app.services.llm.manager import llm_manager  # ‚Üê –î–û–ë–ê–í–ò–¢–¨

logger = logging.getLogger(__name__)
router = APIRouter()
HTTP_TIMEOUT_MODELS = httpx.Timeout(10.0, connect=3.0)


@router.get("/list")
async def list_models(mode: str = "local") -> Dict[str, Any]:
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        logger.info(f"üìã Getting models for mode: {mode}")

        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ 'local', 'ollama' –∏ 'aihub'
        if mode in ["local", "ollama"]:
            # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Ollama
            ollama_url = str(settings.EMBEDDINGS_BASEURL).rstrip('/')
            logger.info(f"üîå Querying Ollama: {ollama_url}")

            try:
                async with httpx.AsyncClient(timeout=HTTP_TIMEOUT_MODELS) as client:
                    response = await client.get(f"{ollama_url}/api/tags")
                    response.raise_for_status()
                    data = response.json()
                models = data.get("models", [])
                logger.info(f"‚úÖ Got {len(models)} models from Ollama")

                return {
                    "mode": mode,
                    "models": [{"name": m.get("name"), "size": m.get("size", 0)} for m in models],
                    "count": len(models)
                }
            except Exception as e:
                logger.error(f"‚ùå Error querying Ollama: {e}")
                return {
                    "mode": mode,
                    "models": [],
                    "count": 0,
                    "error": str(e)
                }

        elif mode in ["aihub", "corporate"]:
            logger.info(f"üè¢ Querying AI HUB for models")

            try:
                logger.info("üì° Using llm_manager to get detailed AI HUB models...")
                detailed_models = await llm_manager.get_available_models_detailed(source="aihub")

                if detailed_models:
                    aihub_models = []
                    for model in detailed_models:
                        aihub_models.append(
                            {
                                "name": model.get("name"),
                                "size": 0,
                                "context_window": model.get("context_window"),
                                "max_output_tokens": model.get("max_output_tokens"),
                            }
                        )
                    logger.info(f"‚úÖ Got {len(aihub_models)} models from AI HUB via llm_manager")
                else:
                    logger.warning("‚ö†Ô∏è No models returned from AI HUB, using defaults")
                    aihub_models = [
                        {"name": "vikhr", "size": 0, "context_window": None, "max_output_tokens": None},
                        {"name": "gpt-4", "size": 0, "context_window": None, "max_output_tokens": None}
                    ]

                return {
                    "mode": mode,
                    "models": aihub_models,
                    "count": len(aihub_models)
                }

            except Exception as e:
                logger.error(f"‚ùå Error getting AI HUB models: {e}", exc_info=True)
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º default –º–æ–¥–µ–ª–∏ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                return {
                    "mode": mode,
                    "models": [
                        {"name": "vikhr", "size": 0},
                        {"name": "gpt-4", "size": 0}
                    ],
                    "count": 2,
                    "error": str(e)
                }

        # OpenAI –∏ –¥—Ä—É–≥–∏–µ —Ä–µ–∂–∏–º—ã
        elif mode == "openai":
            logger.info(f"üîå Using OpenAI models")
            openai_models = [
                {"name": "gpt-4", "size": 0},
                {"name": "gpt-3.5-turbo", "size": 0},
                {"name": "gpt-4-turbo", "size": 0}
            ]
            return {
                "mode": mode,
                "models": openai_models,
                "count": len(openai_models)
            }

        else:
            logger.warning(f"‚ö†Ô∏è Unknown mode: {mode}")
            return {"mode": mode, "models": [], "count": 0}

    except Exception as e:
        logger.error(f"‚ùå Error getting models: {e}", exc_info=True)
        return {"mode": mode, "models": [], "count": 0, "error": str(e)}


@router.get("/status")
async def models_status() -> Dict[str, Any]:
    """–°—Ç–∞—Ç—É—Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
    ollama_available = False
    aihub_available = False
    openai_available = False

    try:
        ollama_url = str(settings.EMBEDDINGS_BASEURL).rstrip('/')
        async with httpx.AsyncClient(timeout=HTTP_TIMEOUT_MODELS) as client:
            response = await client.get(f"{ollama_url}/api/tags")
            ollama_available = response.status_code == 200
    except:
        pass

    # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ AI HUB
    try:
        aihub_url = getattr(settings, 'AIHUB_URL', None)
        aihub_available = bool(aihub_url)
    except:
        pass

    # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ OpenAI
    try:
        openai_api_key = getattr(settings, 'OPENAI_API_KEY', None)
        openai_available = bool(openai_api_key)
    except:
        pass

    return {
        "ollama": {
            "available": ollama_available,
            "url": str(settings.EMBEDDINGS_BASEURL)
        },
        "aihub": {
            "available": aihub_available,
            "url": getattr(settings, 'AIHUB_URL', None)
        },
        "openai": {
            "available": openai_available
        }
    }
