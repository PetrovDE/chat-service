# app/rag/retriever.py
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from langchain_core.documents import Document
from app.rag.vector_store import VectorStoreManager
from app.rag.document_loader import DocumentLoader
from app.rag.text_splitter import SmartTextSplitter
from app.rag.config import rag_config
from app.llm_manager import llm_manager

logger = logging.getLogger(__name__)


class RAGRetriever:
    """
    RAG (Retrieval-Augmented Generation) Retriever
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–æ–≤ LLM
    """

    def __init__(
            self,
            vector_store: VectorStoreManager = None,
            document_loader: DocumentLoader = None,
            text_splitter: SmartTextSplitter = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG retriever

        Args:
            vector_store: –ú–µ–Ω–µ–¥–∂–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
            document_loader: –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            text_splitter: Text splitter
        """
        self.vector_store = vector_store or VectorStoreManager()
        self.document_loader = document_loader or DocumentLoader()
        self.text_splitter = text_splitter or SmartTextSplitter()

        logger.info("‚úÖ RAGRetriever initialized")

    async def process_and_store_file(
            self,
            file_path: str,
            metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ vector store

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        try:
            logger.info(f"üìÇ Processing file for RAG: {file_path}")

            # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
            documents = self.document_loader.load_file(file_path, metadata)

            if not documents:
                raise ValueError("No documents loaded from file")

            # 2. –†–∞–∑–±–∏—Ç—å –Ω–∞ chunks
            chunks = self.text_splitter.split_documents(documents)

            if not chunks:
                raise ValueError("No chunks created from documents")

            # 3. –î–æ–±–∞–≤–∏—Ç—å –≤ vector store
            chunk_ids = self.vector_store.add_documents(chunks)

            # 4. –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = self.text_splitter.get_chunk_stats(chunks)

            result = {
                'success': True,
                'documents_count': len(documents),
                'chunks_count': len(chunks),
                'chunk_ids': chunk_ids,
                'stats': stats
            }

            logger.info(
                f"‚úÖ File processed: {len(documents)} docs ‚Üí {len(chunks)} chunks"
            )
            return result

        except Exception as e:
            logger.error(f"‚ùå Error processing file: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    async def process_and_store_db_file(self, db_file) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–∑ –ë–î –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ vector store

        Args:
            db_file: –û–±—ä–µ–∫—Ç File –∏–∑ –ë–î

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        try:
            logger.info(f"üìÇ Processing DB file: {db_file.original_filename}")

            # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –ë–î
            documents = self.document_loader.load_from_db_file(db_file)

            if not documents:
                raise ValueError("No documents loaded from DB file")

            # 2. –†–∞–∑–±–∏—Ç—å –Ω–∞ chunks —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
            file_type = db_file.file_type.replace('/', '').replace('application', '')

            # –ï—Å–ª–∏ file_type —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            if '.' in db_file.original_filename:
                ext = '.' + db_file.original_filename.split('.')[-1]
                chunks = self.text_splitter.split_by_file_type(
                    documents[0].page_content,
                    ext,
                    metadata=documents[0].metadata
                )
            else:
                chunks = self.text_splitter.split_documents(documents)

            # 3. –î–æ–±–∞–≤–∏—Ç—å file_id –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è
            for chunk in chunks:
                chunk.metadata['file_id'] = str(db_file.id)

            # 4. –î–æ–±–∞–≤–∏—Ç—å –≤ vector store
            chunk_ids = self.vector_store.add_documents(chunks)

            # 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = self.text_splitter.get_chunk_stats(chunks)

            result = {
                'success': True,
                'file_id': str(db_file.id),
                'filename': db_file.original_filename,
                'documents_count': len(documents),
                'chunks_count': len(chunks),
                'chunk_ids': chunk_ids,
                'stats': stats
            }

            logger.info(
                f"‚úÖ DB file processed: {db_file.original_filename} ‚Üí {len(chunks)} chunks"
            )
            return result

        except Exception as e:
            logger.error(f"‚ùå Error processing DB file: {e}")
            return {
                'success': False,
                'file_id': str(db_file.id) if db_file else None,
                'error': str(e)
            }

    def retrieve_context(
            self,
            query: str,
            k: int = None,
            filter: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            filter: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            k = k or rag_config.top_k

            logger.info(f"üîç Retrieving context for query: {query[:50]}...")

            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            results = self.vector_store.similarity_search_with_score(
                query=query,
                k=k,
                filter=filter
            )

            # –ò–∑–≤–ª–µ—á—å —Ç–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–±–µ–∑ scores)
            documents = [doc for doc, score in results]

            logger.info(f"‚úÖ Retrieved {len(documents)} relevant documents")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error retrieving context: {e}")
            return []

    def build_context_prompt(
            self,
            query: str,
            context_documents: List[Document],
            include_metadata: bool = None
    ) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_documents: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            include_metadata: –í–∫–ª—é—á–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

        Returns:
            –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
        """
        include_metadata = include_metadata if include_metadata is not None else rag_config.include_metadata

        if not context_documents:
            # –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –æ–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            return query

        # –°–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []

        for idx, doc in enumerate(context_documents, 1):
            # –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            content = doc.page_content

            # –°–æ–∫—Ä–∞—Ç–∏—Ç—å –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            max_length = rag_config.max_context_length // len(context_documents)
            if len(content) > max_length:
                content = content[:max_length] + "..."

            # –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if include_metadata and doc.metadata:
                source = doc.metadata.get('file_name', doc.metadata.get('source', 'Unknown'))
                context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {idx} –∏–∑ {source}]\n{content}")
            else:
                context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {idx}]\n{content}")

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n---\n\n".join(context_parts)

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–î–û–ö–£–ú–ï–ù–¢–´:
{context}

---

–í–û–ü–†–û–°: {query}

–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—ã—à–µ –¥–ª—è –æ—Ç–≤–µ—Ç–∞. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏. –û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ."""

        return prompt

    async def generate_answer(
            self,
            query: str,
            filter: Optional[Dict[str, Any]] = None,
            temperature: float = 0.7,
            max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è RAG

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filter: –§–∏–ª—å—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            logger.info(f"ü§ñ Generating RAG answer for: {query[:50]}...")

            # 1. –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_docs = self.retrieve_context(query, filter=filter)

            # 2. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç
            prompt = self.build_context_prompt(query, context_docs)

            # 3. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM Manager
            result = await llm_manager.generate_response(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # 4. –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            result['rag_context'] = {
                'documents_used': len(context_docs),
                'sources': [
                    doc.metadata.get('file_name', doc.metadata.get('source', 'Unknown'))
                    for doc in context_docs
                ]
            }

            logger.info(
                f"‚úÖ RAG answer generated using {len(context_docs)} documents"
            )
            return result

        except Exception as e:
            logger.error(f"‚ùå Error generating RAG answer: {e}")
            raise

    async def generate_answer_stream(
            self,
            query: str,
            filter: Optional[Dict[str, Any]] = None,
            temperature: float = 0.7,
            max_tokens: int = 2000
    ) -> AsyncGenerator[str, None]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å streaming –∏—Å–ø–æ–ª—å–∑—É—è RAG

        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            filter: –§–∏–ª—å—Ç—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤

        Yields:
            Chunks –æ—Ç–≤–µ—Ç–∞
        """
        try:
            logger.info(f"ü§ñ Generating streaming RAG answer for: {query[:50]}...")

            # 1. –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_docs = self.retrieve_context(query, filter=filter)

            # 2. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç
            prompt = self.build_context_prompt(query, context_docs)

            # 3. Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ LLM Manager
            async for chunk in llm_manager.generate_response_stream(
                    prompt=prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
            ):
                yield chunk

            logger.info(
                f"‚úÖ RAG streaming completed using {len(context_docs)} documents"
            )

        except Exception as e:
            logger.error(f"‚ùå Error in RAG streaming: {e}")
            raise

    async def analyze_file(
            self,
            file_id: str,
            query: Optional[str] = None
    ) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é RAG

        Args:
            file_id: ID —Ñ–∞–π–ª–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
            query: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å (–µ—Å–ª–∏ None - –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑)

        Returns:
            –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
        """
        try:
            logger.info(f"üìä Analyzing file: {file_id}")

            # –§–∏–ª—å—Ç—Ä –ø–æ file_id
            filter_dict = {'file_id': file_id}

            # –ó–∞–ø—Ä–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if not query:
                query = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å:
1. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
2. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã
3. –°—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
4. –ü–æ–ª–µ–∑–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–ª–∏ –∏–Ω—Å–∞–π—Ç—ã"""

            # –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞
            context_docs = self.retrieve_context(query, filter=filter_dict)

            if not context_docs:
                return "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."

            # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            prompt = self.build_context_prompt(query, context_docs)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞
            result = await llm_manager.generate_response(
                prompt=prompt,
                temperature=0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                max_tokens=2000
            )

            logger.info(f"‚úÖ File analysis completed")
            return result['response']

        except Exception as e:
            logger.error(f"‚ùå Error analyzing file: {e}")
            raise

    def remove_file_from_store(self, file_id: str) -> int:
        """
        –£–¥–∞–ª–∏—Ç—å –≤—Å–µ chunks —Ñ–∞–π–ª–∞ –∏–∑ vector store

        Args:
            file_id: ID —Ñ–∞–π–ª–∞

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö chunks
        """
        try:
            logger.info(f"üóëÔ∏è Removing file from vector store: {file_id}")

            count = self.vector_store.delete_by_filter({'file_id': file_id})

            logger.info(f"‚úÖ Removed {count} chunks for file {file_id}")
            return count

        except Exception as e:
            logger.error(f"‚ùå Error removing file: {e}")
            raise

    def get_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É RAG —Å–∏—Å—Ç–µ–º—ã

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        try:
            vector_stats = self.vector_store.get_collection_stats()

            stats = {
                'vector_store': vector_stats,
                'config': {
                    'chunk_size': rag_config.chunk_size,
                    'chunk_overlap': rag_config.chunk_overlap,
                    'top_k': rag_config.top_k,
                    'similarity_threshold': rag_config.similarity_threshold,
                    'embeddings_model': rag_config.embeddings_model
                }
            }

            logger.info(f"üìä RAG stats: {stats}")
            return stats

        except Exception as e:
            logger.error(f"‚ùå Error getting stats: {e}")
            return {'error': str(e)}


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG retriever
rag_retriever = RAGRetriever()