# Llama Chat Service

<div align="center">

![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.11+-green.svg)
![License](https://img.shields.io/badge/license-MIT-purple.svg)
![Platform](https://img.shields.io/badge/platform-Linux%20%7C%20RedOS8-lightgrey.svg)

**ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‡Ğ°Ñ‚-ÑĞµÑ€Ğ²Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ñ‹Ñ… LLM Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹**

[Features](#features) â€¢ [Installation](#installation) â€¢ [Documentation](#documentation) â€¢ [API](#api) â€¢ [Contributing](#contributing)

</div>

## ğŸŒŸ Features

- ğŸ¦™ **Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ollama** - ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ²Ğ°Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- â˜ï¸ **ĞšĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ API** - ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° OpenAI, Claude, Custom endpoints
- ğŸ“Š **ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²** - Excel, CSV, JSON, TXT Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ°Ğ·Ğ±Ğ¾Ñ€Ğ¾Ğ¼
- ğŸ”„ **Ğ“Ğ¾Ñ€ÑÑ‡ĞµĞµ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ** - ĞœĞµĞ¶Ğ´Ñƒ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°
- ğŸš€ **Streaming Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹** - Ğ ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‡ĞµÑ€ĞµĞ· SSE
- ğŸ”’ **Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ** - Ğ˜Ğ·Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, systemd hardening, HTTPS
- ğŸ“ˆ **ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³** - Ğ’ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğµ health checks Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸

## ğŸ›  Tech Stack

- **Backend**: FastAPI, LangChain, Ollama
- **Frontend**: Vanilla JS, Modern CSS
- **Deployment**: systemd, Nginx, RedOS8
- **Models**: Llama 3.1 8B (default), GPT-4, Claude (optional)

## ğŸ“‹ Requirements

### System Requirements
- **OS**: RedOS8, RHEL 8+, CentOS 8+, Ubuntu 20.04+
- **Python**: 3.11+
- **RAM**: 16GB minimum (8GB for model + 8GB system)
- **Storage**: 20GB+ free space
- **CPU**: 4+ cores recommended

### Software Dependencies
- Ollama (for local models)
- Nginx (reverse proxy)
- systemd (service management)

## ğŸš€ Quick Start

### Local Development

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/llama-chat-service.git
cd llama-chat-service

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Setup Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull llama3.1:8b

# Configure environment
cp config/.env.example .env
# Edit .env with your settings

# Run development server
python app/main.py
```

Open http://localhost:8000 in your browser.

## ğŸ“¦ Production Deployment

### 1. Automated Installation

```bash
# On your RedOS8/RHEL server
git clone https://github.com/YOUR_USERNAME/llama-chat-service.git
cd llama-chat-service
chmod +x scripts/*.sh

# Run installation script
./scripts/install.sh

# Setup systemd service
sudo ./scripts/setup_systemd.sh

# Configure Nginx (edit domain in config)
sudo cp config/nginx.conf.example /etc/nginx/conf.d/llama-chat.conf
sudo nano /etc/nginx/conf.d/llama-chat.conf
sudo systemctl restart nginx
```

### 2. Manual Installation

See [Deployment Guide](docs/DEPLOYMENT.md) for detailed instructions.

## ğŸ”§ Configuration

### Environment Variables

```bash
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Server Configuration  
HOST=0.0.0.0
PORT=8000
WORKERS=4

# API Configuration (optional)
CORPORATE_API_URL=https://api.openai.com/v1/chat/completions
CORPORATE_API_KEY=sk-...
CORPORATE_API_TYPE=openai
CORPORATE_MODEL_NAME=gpt-4

# Security
SECRET_KEY=your-secret-key-here
ALLOWED_ORIGINS=http://localhost:8000,https://your-domain.com
```

### Model Sources

#### Local Models (Ollama)
```bash
# List available models
ollama list

# Pull new model
ollama pull mistral:7b
ollama pull codellama:13b
```

#### Corporate API
Configure through UI or API:
```json
{
  "source": "api",
  "api_config": {
    "api_url": "https://api.openai.com/v1/chat/completions",
    "api_key": "sk-...",
    "model_name": "gpt-4",
    "api_type": "openai"
  }
}
```

## ğŸ“– API Documentation

### Chat Endpoints

#### Send Message
```bash
POST /chat
Content-Type: application/json

{
  "message": "Hello, how are you?",
  "temperature": 0.7,
  "max_tokens": 1000
}
```

#### Stream Response
```bash
POST /chat/stream
Content-Type: application/json

{
  "message": "Tell me a story",
  "temperature": 0.8
}
```

### File Analysis

#### Upload File
```bash
POST /upload
Content-Type: multipart/form-data

file: <binary>
```

#### Analyze File
```bash
POST /analyze-file
Content-Type: application/json

{
  "content": "file content...",
  "filename": "data.xlsx",
  "analysis_type": "summary",
  "custom_prompt": null
}
```

### Model Management

#### Switch Model Source
```bash
POST /api/source
Content-Type: application/json

{
  "source": "local" | "api",
  "api_config": {...}  # if source="api"
}
```

#### List Local Models
```bash
GET /api/models/local
```

#### Health Check
```bash
GET /health
```

Full API documentation available at `/docs` when service is running.

## ğŸ—‚ Project Structure

```
llama-chat-service/
â”œâ”€â”€ app/                    # Application code
â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â”œâ”€â”€ llm_service.py     # Ollama integration
â”‚   â”œâ”€â”€ llm_manager.py     # Model source manager
â”‚   â”œâ”€â”€ api_llm_service.py # Corporate API integration
â”‚   â””â”€â”€ static/            # Frontend files
â”‚       â”œâ”€â”€ index.html
â”‚       â”œâ”€â”€ styles.css
â”‚       â””â”€â”€ js/
â”œâ”€â”€ config/                 # Configuration examples
â”‚   â”œâ”€â”€ nginx.conf.example
â”‚   â”œâ”€â”€ systemd.service.example
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ scripts/               # Automation scripts
â”‚   â”œâ”€â”€ install.sh
â”‚   â”œâ”€â”€ setup_systemd.sh
â”‚   â””â”€â”€ update.sh
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md
```

## ğŸ”’ Security Features

- **Data Privacy**: All data processed locally when using Ollama
- **HTTPS**: Enforced in production with SSL/TLS
- **systemd Hardening**: PrivateTmp, NoNewPrivileges
- **Input Validation**: Pydantic models for all endpoints
- **CORS**: Configurable allowed origins
- **File Upload**: Type and size restrictions
- **API Keys**: Secure storage, never logged

## ğŸ› Troubleshooting

### Common Issues

#### Ollama Connection Error
```bash
# Check Ollama service
systemctl status ollama
ollama list

# Restart Ollama
sudo systemctl restart ollama
```

#### 502 Bad Gateway
```bash
# Check app service
systemctl status llama-chat
journalctl -u llama-chat -n 50

# Check port binding
ss -tlnp | grep :8000
```

#### Model Not Found
```bash
# Pull required model
ollama pull llama3.1:8b

# Verify installation
ollama list
```

See [Troubleshooting Guide](docs/TROUBLESHOOTING.md) for more solutions.

## ğŸ“Š Monitoring

### Service Health
```bash
# Check all services
./scripts/monitor.sh

# View logs
journalctl -u llama-chat -f
tail -f /var/log/llama-chat/service.log
```

### Performance Metrics
```bash
# CPU and Memory usage
htop -p $(pgrep -f "uvicorn")

# API response times
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8000/health
```

## ğŸ”„ Updates

### Automatic Update
```bash
cd /opt/llama-chat
./scripts/update.sh
```

### Manual Update
```bash
# Stop service
sudo systemctl stop llama-chat

# Pull changes
git pull origin main

# Update dependencies
source venv/bin/activate
pip install -r requirements.txt --upgrade

# Restart service
sudo systemctl start llama-chat
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](docs/CONTRIBUTING.md) for details.

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai) - Local model runtime
- [FastAPI](https://fastapi.tiangolo.com) - Modern web framework
- [LangChain](https://langchain.com) - LLM orchestration
- [Meta AI](https://ai.meta.com) - Llama models

## ğŸ“® Support

- **Issues**: [GitHub Issues](https://github.com/YOUR_USERNAME/llama-chat-service/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_USERNAME/llama-chat-service/discussions)
- **Wiki**: [Documentation Wiki](https://github.com/YOUR_USERNAME/llama-chat-service/wiki)

## ğŸš¦ Status

- âœ… Production Ready
- âœ… Active Development
- âœ… Security Updates

---

<div align="center">
Made with â¤ï¸ for enterprise AI deployment
</div>