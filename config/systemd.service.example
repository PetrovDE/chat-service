[Unit]
Description=Llama Chat Service - AI Chat Application
Documentation=https://github.com/YOUR_USERNAME/llama-chat-service
After=network-online.target ollama.service
Wants=network-online.target
Requires=ollama.service

[Service]
Type=simple
User=llama-chat
Group=llama-chat
WorkingDirectory=/opt/llama-chat

# Environment variables
Environment="PATH=/opt/llama-chat/venv/bin:/usr/local/bin:/usr/bin:/bin"
Environment="PYTHONPATH=/opt/llama-chat"
Environment="LANG=en_US.UTF-8"
Environment="LC_ALL=en_US.UTF-8"

# Load environment file
EnvironmentFile=/opt/llama-chat/.env

# Main process
ExecStart=/opt/llama-chat/venv/bin/python -m uvicorn app.main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4 \
    --loop uvloop \
    --access-log \
    --use-colors

# Process management
Restart=always
RestartSec=10
KillMode=mixed
KillSignal=SIGTERM
TimeoutStopSec=30

# Security hardening
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/var/lib/llama-chat /var/log/llama-chat
ProtectKernelTunables=true
ProtectKernelModules=true
ProtectControlGroups=true
RestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX
RestrictNamespaces=true
RestrictRealtime=true
RestrictSUIDSGID=true
LockPersonality=true
SystemCallFilter=@system-service
SystemCallErrorNumber=EPERM

# Resource limits
LimitNOFILE=65536
LimitNPROC=4096
LimitCORE=0
MemoryLimit=4G
CPUQuota=200%

# Logging
StandardOutput=append:/var/log/llama-chat/service.log
StandardError=append:/var/log/llama-chat/error.log
SyslogIdentifier=llama-chat

# Health check
ExecStartPre=/opt/llama-chat/venv/bin/python -c "import app.main"
ExecReload=/bin/kill -USR1 $MAINPID

[Install]
WantedBy=multi-user.target