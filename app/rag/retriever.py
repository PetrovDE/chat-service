# app/rag/retriever.py
import logging
from typing import List, Optional, Dict, Any
from langchain_core.documents import Document

from app.rag.vector_store import VectorStoreManager
from app.rag.document_loader import DocumentLoader
from app.rag.text_splitter import SmartTextSplitter
from app.rag.embeddings import embeddings_manager

logger = logging.getLogger(__name__)


class RAGRetriever:
    def __init__(
            self,
            vectorstore: Optional[VectorStoreManager] = None,
            documentloader: Optional[DocumentLoader] = None,
            textsplitter: Optional[SmartTextSplitter] = None
    ):
        self.vectorstore = vectorstore or VectorStoreManager()
        self.documentloader = documentloader or DocumentLoader()
        self.textsplitter = textsplitter or SmartTextSplitter()
        logger.info("‚úÖ RAGRetriever initialized")

    async def process_and_store_file(
            self,
            filepath: str,
            metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        """
        logger.info(f"üìÇ Processing file: {filepath}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        docs = await self.documentloader.load_file(filepath, metadata)
        logger.info(f"üìÑ Loaded {len(docs)} documents from file")

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunk_docs = self.textsplitter.split_documents(docs)
        logger.info(f"‚úÇÔ∏è Split into {len(chunk_docs)} chunks")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º –∏ –º–æ–¥–µ–ª—å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        current_mode = embeddings_manager.mode
        current_model = embeddings_manager.model
        expected_dim = embeddings_manager.get_embedding_dimension()

        logger.info(
            f"üîÆ Generating embeddings: mode={embeddings_manager.original_mode}, "
            f"model={'arctic' if current_mode == 'aihub' else current_model}, "
            f"expected_dim={expected_dim}"
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = await embeddings_manager.embedd_documents_async([doc.page_content for doc in chunk_docs])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        for doc, emb in zip(chunk_docs, embeddings):
            chunk_id = doc.metadata.get("id", None) or doc.page_content[:40]

            # ‚úÖ –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            doc.metadata['embedding_model'] = 'arctic' if current_mode == 'aihub' else current_model
            doc.metadata['embedding_mode'] = embeddings_manager.original_mode
            doc.metadata['embedding_dimension'] = len(emb)

            self.vectorstore.add_document(
                doc_id=chunk_id,
                embedding=emb,
                metadata=doc.metadata
            )

        logger.info(
            f"‚úÖ Stored {len(chunk_docs)} chunks with {len(embeddings[0]) if embeddings else 0}d embeddings"
        )

        return {
            "count_stored_chunks": len(chunk_docs),
            "embedding_dimension": len(embeddings[0]) if embeddings else 0,
            "embedding_model": 'arctic' if current_mode == 'aihub' else current_model,
            "embedding_mode": embeddings_manager.original_mode
        }

    async def query_rag(
            self,
            query_content: str,
            top_k: int = 5,
            user_id: Optional[str] = None,
            conversation_id: Optional[str] = None,
            model_source: Optional[str] = None  # ‚úÖ –ù–û–í–´–ô –ü–ê–†–ê–ú–ï–¢–†
    ) -> List[Document]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ RAG

        Args:
            query_content: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            conversation_id: ID –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            model_source: –ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        """
        # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º
        original_mode = embeddings_manager.original_mode
        original_model = embeddings_manager.model

        try:
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Ä–µ–∂–∏–º, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ñ–∞–π–ª–æ–≤
            if model_source:
                if model_source in ['corporate', 'aihub']:
                    # –î–ª—è AI HUB –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º arctic
                    embeddings_manager.switch_mode('corporate')
                    logger.info("üîÑ Switched to AI HUB mode (arctic) for query embedding")
                else:
                    embeddings_manager.switch_mode(model_source)
                    logger.info(f"üîÑ Switched to {model_source} mode for query embedding")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ
            expected_dim = embeddings_manager.get_embedding_dimension()
            logger.info(
                f"üîÆ Generating query embedding: mode={embeddings_manager.original_mode}, "
                f"expected_dim={expected_dim}"
            )

            embedding_query = (await embeddings_manager.embedd_documents_async([query_content]))[0]
            actual_dim = len(embedding_query)

            logger.info(
                f"‚úÖ Query embedding generated: {actual_dim}d "
                f"(expected: {expected_dim}d)"
            )

            if actual_dim != expected_dim:
                logger.warning(
                    f"‚ö†Ô∏è Dimension mismatch: expected {expected_dim}, got {actual_dim}"
                )

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self.vectorstore.query(embedding_query, top_k=top_k * 2)  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

            logger.info(f"üîç Vector store returned {len(results)} raw results")

            # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ conversation_id (–ì–õ–ê–í–ù–û–ï!)
            if conversation_id:
                filtered_results = [
                    r for r in results
                    if r.get('metadata', {}).get('conversation_id') == conversation_id
                ]
                logger.info(
                    f"üîç After conversation_id filter ({conversation_id}): "
                    f"{len(filtered_results)} results"
                )
                results = filtered_results

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ user_id (–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
            if user_id:
                filtered_results = [
                    r for r in results
                    if r.get('metadata', {}).get('user_id') == user_id
                ]
                logger.info(f"üîç After user_id filter: {len(filtered_results)} results")
                results = filtered_results

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ top_k
            results = results[:top_k]

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Document
            documents = []
            for idx, result in enumerate(results):
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ content
                content = None

                if 'content' in result and result['content']:
                    content = result['content']
                elif 'metadata' in result and 'content' in result['metadata']:
                    content = result['metadata']['content']
                elif 'metadata' in result and 'page_content' in result['metadata']:
                    content = result['metadata']['page_content']
                elif 'metadata' in result and 'text' in result['metadata']:
                    content = result['metadata']['text']

                if not content or not str(content).strip():
                    logger.warning(
                        f"‚ö†Ô∏è Empty content for result {idx}, id={result.get('id')}, "
                        f"metadata keys: {list(result.get('metadata', {}).keys())}"
                    )
                    continue

                metadata = result.get('metadata', {}).copy()
                metadata['result_index'] = idx
                metadata['similarity_score'] = result.get('distance', 0)

                doc = Document(
                    page_content=str(content),
                    metadata=metadata
                )
                documents.append(doc)

                logger.debug(
                    f"‚úÖ Document {idx}: {len(content)} chars, "
                    f"file={metadata.get('filename', 'unknown')}, "
                    f"conv_id={metadata.get('conversation_id', 'none')}, "
                    f"embedding_dim={metadata.get('embedding_dimension', 'unknown')}"
                )

            logger.info(f"‚úÖ Returning {len(documents)} valid documents for RAG")
            if not documents:
                logger.warning("‚ö†Ô∏è No valid documents found - RAG context will be empty!")

            return documents

        finally:
            # ‚úÖ –ö–†–ò–¢–ò–ß–ù–û: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º
            embeddings_manager.switch_mode(original_mode)
            if original_model:
                embeddings_manager.switch_model(original_model)
            logger.info(f"üîÑ Restored original mode: {original_mode}")

    def build_context_prompt(self, query: str, context_documents: List[Document]) -> str:
        """
        –°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not context_documents:
            logger.warning("‚ö†Ô∏è No context documents provided - using query only")
            return query

        context_chunks = []
        for i, doc in enumerate(context_documents, 1):
            content = doc.page_content
            if not content or not content.strip():
                logger.warning(f"‚ö†Ô∏è Skipping document {i} - empty content")
                continue

            filename = doc.metadata.get('filename', 'Unknown')
            file_type = doc.metadata.get('file_type', '')
            chunk_index = doc.metadata.get('chunk_index', '')

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_header = f"[–î–æ–∫—É–º–µ–Ω—Ç {i} - {filename}"
            if file_type:
                doc_header += f" ({file_type})"
            if chunk_index is not None and chunk_index != '':
                doc_header += f" - —á–∞—Å—Ç—å {chunk_index + 1}"
            doc_header += "]"

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            max_content_length = 2000
            if len(content) > max_content_length:
                content = content[:max_content_length] + "\n[... —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ–±—Ä–µ–∑–∞–Ω–æ ...]"

            context_chunks.append(f"{doc_header}\n{content}")

        if not context_chunks:
            logger.warning("‚ö†Ô∏è All documents were empty - using query only")
            return query

        context_text = "\n\n---\n\n".join(context_chunks)

        prompt = f"""–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ.

–ö–û–ù–¢–ï–ö–°–¢ ({len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤):
{context_text}

–í–û–ü–†–û–°:
{query}

–û–¢–í–ï–¢:"""

        logger.info(
            f"üìù Built prompt: {len(context_documents)} docs, "
            f"{len(context_text)} context chars, "
            f"{len(prompt)} total chars"
        )

        return prompt


rag_retriever = RAGRetriever()
