"""
LLM Manager with Provider Architecture
–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ LLM
"""
import logging
from typing import Optional, Dict, Any, List, AsyncGenerator

from app.core.config import settings
from app.services.llm.providers.base import BaseLLMProvider
from app.services.llm.providers.ollama import ollama_provider
from app.services.llm.providers.openai import openai_provider
from app.services.llm.providers.aihub import aihub_provider

logger = logging.getLogger(__name__)


class LLMManager:
    """
    Unified LLM Manager with Provider Architecture
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Ollama (local), OpenAI, AI HUB
    """

    def __init__(self):
        self.default_source = settings.DEFAULT_MODEL_SOURCE

        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        self.providers: Dict[str, BaseLLMProvider] = {
            "ollama": ollama_provider,
            "local": ollama_provider,  # –ê–ª–∏–∞—Å –¥–ª—è ollama
            "openai": openai_provider,
            "aihub": aihub_provider,
        }

        logger.info(f"üöÄ LLMManager initialized with providers: {list(self.providers.keys())}")
        logger.info(f"üìå Default source: {self.default_source}")

    def _get_provider(self, source: str) -> BaseLLMProvider:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ –∏–º–µ–Ω–∏"""
        provider = self.providers.get(source)
        if not provider:
            raise ValueError(f"Unknown model source: {source}. Available: {list(self.providers.keys())}")
        return provider

    async def get_available_models(self, source: str = "ollama") -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        try:
            provider = self._get_provider(source)
            models = await provider.get_available_models()
            logger.info(f"üìã Models from {source}: {models}")
            return models
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch models from {source}: {e}")
            return []

    async def generate_response(
            self,
            prompt: str,
            model_source: Optional[str] = None,
            model_name: Optional[str] = None,
            temperature: float = 0.7,
            max_tokens: int = 2000,
            conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (–±–µ–∑ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞)"""
        source = model_source or self.default_source
        provider = self._get_provider(source)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
        if not model_name:
            if source == "ollama" or source == "local":
                # FIX: —á–∞—Ç-–º–æ–¥–µ–ª—å != embedding-–º–æ–¥–µ–ª—å
                model_name = settings.OLLAMA_CHAT_MODEL or settings.EMBEDDINGS_MODEL
            elif source == "openai":
                model_name = settings.OPENAI_MODEL
            elif source == "aihub":
                model_name = settings.AIHUB_DEFAULT_MODEL

        logger.info(f"üîß Generating response: source={source}, model={model_name}")

        return await provider.generate_response(
            prompt=prompt,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            conversation_history=conversation_history
        )

    async def generate_response_stream(
            self,
            prompt: str,
            model_source: Optional[str] = None,
            model_name: Optional[str] = None,
            temperature: float = 0.7,
            max_tokens: int = 2000,
            conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º"""
        source = model_source or self.default_source
        provider = self._get_provider(source)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
        if not model_name:
            if source == "ollama" or source == "local":
                # FIX: —á–∞—Ç-–º–æ–¥–µ–ª—å != embedding-–º–æ–¥–µ–ª—å
                model_name = settings.OLLAMA_CHAT_MODEL or settings.EMBEDDINGS_MODEL
            elif source == "openai":
                model_name = settings.OPENAI_MODEL
            elif source == "aihub":
                model_name = settings.AIHUB_DEFAULT_MODEL

        logger.info(f"üîß Streaming response: source={source}, model={model_name}")

        async for chunk in provider.generate_response_stream(
            prompt=prompt,
            model=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            conversation_history=conversation_history
        ):
            yield chunk

    async def generate_embedding(
            self,
            text: str,
            model_source: Optional[str] = None,
            model_name: Optional[str] = None
    ) -> Optional[List[float]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥"""
        source = model_source or self.default_source
        provider = self._get_provider(source)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if not model_name:
            if source == "aihub":
                model_name = settings.AIHUB_EMBEDDING_MODEL
            elif source == "ollama" or source == "local":
                # FIX: —Ä–∞–Ω—å—à–µ —Ç—É—Ç –æ—Å—Ç–∞–≤–∞–ª–æ—Å—å None ‚Üí –∏ —ç—Ç–æ –ª–æ–º–∞–ª–æ retrieval
                model_name = settings.OLLAMA_EMBED_MODEL or settings.EMBEDDINGS_MODEL

        logger.info(f"üîÆ Generating embedding: source={source}, model={model_name}")

        return await provider.generate_embedding(
            text=text,
            model=model_name
        )


# Create singleton instance
llm_manager = LLMManager()
