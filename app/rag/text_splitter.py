# app/rag/text_splitter.py
import logging
from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from app.core.config import settings

logger = logging.getLogger(__name__)

MIN_CHUNK_SIZE = 50  # –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
DEFAULT_CHUNK_SIZE = getattr(settings, "CHUNK_SIZE", 1000) or 1000
DEFAULT_CHUNK_OVERLAP = getattr(settings, "CHUNK_OVERLAP", 200) or 200

class SmartTextSplitter:
    def __init__(
        self,
        chunk_size: int = None,
        chunk_overlap: int = None,
        separators: List[str] = None
    ):
        # 1) –ó–∞–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –∫–æ–Ω—Ñ–∏–≥–∞
        raw_chunk_size = chunk_size if chunk_size is not None else DEFAULT_CHUNK_SIZE
        raw_chunk_overlap = chunk_overlap if chunk_overlap is not None else DEFAULT_CHUNK_OVERLAP

        # 2) –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chunk_size (–Ω–µ –¥–∞–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ/–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ)
        try:
            raw_chunk_size = int(raw_chunk_size)
        except Exception:
            logger.warning(f"chunk_size={raw_chunk_size} is not int, fallback to {DEFAULT_CHUNK_SIZE}")
            raw_chunk_size = DEFAULT_CHUNK_SIZE

        if raw_chunk_size < MIN_CHUNK_SIZE:
            logger.warning(f"chunk_size={raw_chunk_size} < {MIN_CHUNK_SIZE}, bumping to {MIN_CHUNK_SIZE}")
            raw_chunk_size = MIN_CHUNK_SIZE

        # 3) –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º overlap: –æ–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å < chunk_size
        try:
            raw_chunk_overlap = int(raw_chunk_overlap)
        except Exception:
            logger.warning(f"chunk_overlap={raw_chunk_overlap} is not int, fallback to {DEFAULT_CHUNK_OVERLAP}")
            raw_chunk_overlap = DEFAULT_CHUNK_OVERLAP

        if raw_chunk_overlap >= raw_chunk_size:
            adjusted = max(0, raw_chunk_size // 4)
            logger.warning(
                f"chunk_overlap={raw_chunk_overlap} >= chunk_size={raw_chunk_size}, "
                f"reducing overlap to {adjusted}"
            )
            raw_chunk_overlap = adjusted

        self.chunk_size = raw_chunk_size
        self.chunk_overlap = raw_chunk_overlap
        self.separators = separators or ["\n\n", "\n", ". ", " ", ""]

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=self.separators,
            length_function=len,
            is_separator_regex=False
        )

        logger.info(
            f"‚úÖ SmartTextSplitter initialized: "
            f"chunk_size={self.chunk_size}, overlap={self.chunk_overlap}"
        )

    def split_text(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ chunks

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö chunks
        """
        try:
            if not text or not text.strip():
                logger.warning("‚ö†Ô∏è Empty text provided for splitting")
                return []

            logger.info(f"üî™ Splitting text ({len(text)} chars)...")

            chunks = self.text_splitter.split_text(text)

            logger.info(f"‚úÖ Text split into {len(chunks)} chunks")
            return chunks

        except Exception as e:
            logger.error(f"‚ùå Error splitting text: {e}")
            raise

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        –†–∞–∑–±–∏—Ç—å —Å–ø–∏—Å–æ–∫ Document –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ chunks
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            documents: –°–ø–∏—Å–æ–∫ LangChain Document –æ–±—ä–µ–∫—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ Document chunks —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            if not documents:
                logger.warning("‚ö†Ô∏è Empty documents list provided")
                return []

            logger.info(f"üî™ Splitting {len(documents)} documents...")

            all_chunks = []

            for doc_idx, doc in enumerate(documents):
                # –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
                text_chunks = self.text_splitter.split_text(doc.page_content)

                # –°–æ–∑–¥–∞—Ç—å Document –æ–±—ä–µ–∫—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk
                for chunk_idx, chunk_text in enumerate(text_chunks):
                    # –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
                    metadata = doc.metadata.copy()

                    # –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ chunk
                    metadata.update({
                        'chunk_index': chunk_idx,
                        'total_chunks': len(text_chunks),
                        'doc_index': doc_idx,
                        'chunk_size': len(chunk_text)
                    })

                    # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π Document
                    chunk_doc = Document(
                        page_content=chunk_text,
                        metadata=metadata
                    )
                    all_chunks.append(chunk_doc)

            logger.info(f"‚úÖ Created {len(all_chunks)} document chunks")
            return all_chunks

        except Exception as e:
            logger.error(f"‚ùå Error splitting documents: {e}")
            raise

    def create_documents_from_text(
            self,
            text: str,
            metadata: Dict[str, Any] = None
    ) -> List[Document]:
        """
        –°–æ–∑–¥–∞—Ç—å —Å–ø–∏—Å–æ–∫ Document –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ Document chunks
        """
        try:
            if not text or not text.strip():
                logger.warning("‚ö†Ô∏è Empty text provided")
                return []

            metadata = metadata or {}

            logger.info(f"üìÑ Creating documents from text ({len(text)} chars)...")

            # –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç
            chunks = self.split_text(text)

            # –°–æ–∑–¥–∞—Ç—å Document –æ–±—ä–µ–∫—Ç—ã
            documents = []
            for idx, chunk in enumerate(chunks):
                doc_metadata = metadata.copy()
                doc_metadata.update({
                    'chunk_index': idx,
                    'total_chunks': len(chunks),
                    'chunk_size': len(chunk)
                })

                doc = Document(
                    page_content=chunk,
                    metadata=doc_metadata
                )
                documents.append(doc)

            logger.info(f"‚úÖ Created {len(documents)} documents")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error creating documents: {e}")
            raise

    def split_by_file_type(
            self,
            text: str,
            file_type: str,
            metadata: Dict[str, Any] = None
    ) -> List[Document]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            file_type: –¢–∏–ø —Ñ–∞–π–ª–∞ (pdf, docx, txt, csv, etc.)
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞

        Returns:
            –°–ø–∏—Å–æ–∫ Document chunks
        """
        try:
            metadata = metadata or {}
            metadata['file_type'] = file_type

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤
            if file_type in ['csv', 'xlsx', 'xls']:
                # –î–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –±–æ–ª—å—à–∏–π chunk size
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=self.chunk_size * 2,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                    chunk_overlap=self.chunk_overlap,
                    separators=["\n\n", "\n"],  # –¢–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏
                    length_function=len
                )
                chunks = splitter.split_text(text)

            elif file_type == 'json':
                # JSON - —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap,
                    separators=["\n\n", "\n", ",", " "],
                    length_function=len
                )
                chunks = splitter.split_text(text)

            elif file_type == 'md':
                # Markdown - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                splitter = RecursiveCharacterTextSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap,
                    separators=["\n## ", "\n### ", "\n\n", "\n", ". ", " "],
                    length_function=len
                )
                chunks = splitter.split_text(text)

            else:
                # –û–±—ã—á–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤
                chunks = self.split_text(text)

            # –°–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            documents = []
            for idx, chunk in enumerate(chunks):
                doc_metadata = metadata.copy()
                doc_metadata.update({
                    'chunk_index': idx,
                    'total_chunks': len(chunks),
                    'chunk_size': len(chunk)
                })

                doc = Document(
                    page_content=chunk,
                    metadata=doc_metadata
                )
                documents.append(doc)

            logger.info(
                f"‚úÖ Split {file_type} file into {len(documents)} chunks "
                f"(avg size: {sum(len(c) for c in chunks) // len(chunks)})"
            )
            return documents

        except Exception as e:
            logger.error(f"‚ùå Error splitting by file type: {e}")
            raise

    def get_chunk_stats(self, documents: List[Document]) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ chunks

        Args:
            documents: –°–ø–∏—Å–æ–∫ Document chunks

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if not documents:
            return {
                'total_chunks': 0,
                'avg_chunk_size': 0,
                'min_chunk_size': 0,
                'max_chunk_size': 0,
                'total_size': 0
            }

        chunk_sizes = [len(doc.page_content) for doc in documents]

        stats = {
            'total_chunks': len(documents),
            'avg_chunk_size': sum(chunk_sizes) // len(chunk_sizes),
            'min_chunk_size': min(chunk_sizes),
            'max_chunk_size': max(chunk_sizes),
            'total_size': sum(chunk_sizes),
            'overlap_ratio': self.chunk_overlap / self.chunk_size
        }

        logger.info(f"üìä Chunk statistics: {stats}")
        return stats
